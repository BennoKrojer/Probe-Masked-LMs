{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "probeBERT.ipynb",
      "provenance": [],
      "mount_file_id": "1x-SJSL-ayVz6lezuATBaPPilUeedtv2_",
      "authorship_tag": "ABX9TyP2zJdzH4BJCMpqsFKH563d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BennoKrojer/Probe-Masked-LMs/blob/master/probeBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnH7YtxQM7Km",
        "colab_type": "code",
        "outputId": "84c73e4f-061c-44a0-8bc1-4cb90e3f6a43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 61.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 46.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 47.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=b64fe9c0fc03ac94804fb91cc4440d520fe65e6c0670bd0838d88f4757377565\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQwkcDpP6fsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "probes_raw = \"\"\"\n",
        "nice is a synonym of\n",
        "good is a synonym of\n",
        "bad is a synonym of\n",
        "sweet is a synonym of\n",
        "red is a synonym of\n",
        "pretty is a synonym of\n",
        "beautiful is a synonym of\n",
        "friendly is a synonym of\n",
        "usual is a synonym of\n",
        "common is a synonym of\n",
        "ordinary is a synonym of\n",
        "\"\"\"\n",
        "oneto1 = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRNTd42zopnR",
        "colab_type": "code",
        "outputId": "e2f7e281-59c8-42bf-dcca-6b2fde92282e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM, RobertaForMaskedLM, RobertaTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# from: https://huggingface.co/transformers/quickstart.html#bert-example\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n",
        "model.eval()\n",
        "\n",
        "previous_preds = []\n",
        "previous_ent = ''\n",
        "both_correct = ''\n",
        "both_correct_c = 0\n",
        "both_false = ''\n",
        "both_false_c = 0\n",
        "asymmetric_false = \"\"\n",
        "asymmetric_c = 0\n",
        "rest = []\n",
        "probes = probes_raw.split('\\n')\n",
        "for i, probe in enumerate(probes):\n",
        "  if len(probe) > 5:\n",
        "    text = f'[CLS] {probe} [MASK] . [SEP]'\n",
        "    tokenized_text = tokenizer.tokenize(text)\n",
        "    masked_index = [i for i, x in enumerate(tokenized_text) if x == '[MASK]'][0]\n",
        "    # print(tokenized_text)\n",
        "\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    segments_ids = [0]*len(tokenized_text)\n",
        "\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "    # Predict all tokens\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "        predictions = outputs[0][0][masked_index]\n",
        "    predicted_ids = torch.argsort(predictions, descending=True)[:20]\n",
        "    predicted_tokens = tokenizer.convert_ids_to_tokens(list(predicted_ids))\n",
        "    if oneto1:\n",
        "      if i % 2 == 1:\n",
        "        previous_preds = predicted_tokens\n",
        "        previous_ent = probe.split(\"'\")[0]\n",
        "      else:\n",
        "        current_ent = probe.split()[0]\n",
        "        current_prediction = predicted_tokens[0] if predicted_tokens[0] != current_ent else predicted_tokens[1] \n",
        "        prev_prediction = previous_preds[0] if previous_preds[0] != previous_ent else previous_preds[1]\n",
        "        if current_ent == prev_prediction and current_prediction == previous_ent:\n",
        "          both_correct += f'{probes[i-1]} [MASK] → {previous_preds}\\n{probe} [MASK] → {predicted_tokens}\\n'\n",
        "          both_correct_c += 1\n",
        "        elif current_ent != prev_prediction and current_prediction != previous_ent:\n",
        "          both_false += f'{probes[i-1]} [MASK] → {previous_preds}\\n{probe} [MASK] → {predicted_tokens}\\n'\n",
        "          both_false_c += 1\n",
        "        else:\n",
        "          asymmetric_false += f'{probes[i-1]} [MASK] → {previous_preds}\\n{probe} [MASK] → {predicted_tokens}\\n'\n",
        "          asymmetric_c += 1\n",
        "    else:\n",
        "      print(f'{probe} [MASK] → {predicted_tokens}')\n",
        "\n",
        "if oneto1:\n",
        "  print(both_correct_c)\n",
        "  print(both_false_c)\n",
        "  print(asymmetric_c)\n",
        "  print('both correct:')\n",
        "  print(both_correct)\n",
        "  print('both false')\n",
        "  print(both_false)\n",
        "  print('asymmetrically false')\n",
        "  print(asymmetric_false)\n",
        "\n",
        "\n",
        "    # print(f'{probe} [MASK] → {predicted_tokens}')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nice is a synonym of [MASK] → ['nice', 'it', 'var', 'ana', 'io', 'cicero', 'minor', 'bella', 'venus', '[UNK]', 'ca', 'this', 'normal', 'homo', 'c', 'p', 'mons', 'florida', 'pseudo', 'nova']\n",
            "good is a synonym of [MASK] → ['good', 'bad', 'excellent', 'perfect', 'great', 'superb', 'nice', 'gold', 'best', 'beautiful', 'fair', 'sir', 'fine', 'poor', 'evil', 'b', 'it', 'venus', 'minor', 'p']\n",
            "bad is a synonym of [MASK] → ['good', 'bad', 'big', 'sir', 'nice', 'mad', 'it', 'red', 'b', 'black', 'la', 'pro', 'great', 'gold', 'perfect', 'old', 'no', 'excellent', 'vice', 'negro']\n",
            "sweet is a synonym of [MASK] → ['sweet', 'red', 'black', 'rose', 'white', 'gray', 'brown', 'good', 'blue', 'love', 'yellow', 'bird', '[UNK]', 'kali', 'jack', 'sugar', 'honey', 'bitter', 'may', 'winter']\n",
            "red is a synonym of [MASK] → ['black', 'blue', 'yellow', 'red', 'white', 'green', 'brown', 'orange', 'gold', 'purple', 'grey', 'gray', 'violet', 'pink', 'dark', 'silver', 'rose', 'ruby', 'pale', 'sapphire']\n",
            "pretty is a synonym of [MASK] → ['bella', 'iris', 'doris', 'it', 'minor', 'io', 'venus', 'diana', 'viola', '[UNK]', 'niger', 'nice', 'montana', 'dora', 'leo', 'delta', 'mona', 'melissa', 'nova', 'nana']\n",
            "beautiful is a synonym of [MASK] → ['beautiful', 'lovely', 'bella', 'beauty', 'gorgeous', 'pretty', 'miss', 'handsome', 'perfect', 'princess', 'venus', 'nice', 'ugly', 'brilliant', 'splendid', 'magnificent', '[UNK]', 'red', 'rose', 'mia']\n",
            "friendly is a synonym of [MASK] → ['[UNK]', 'venus', 'it', 'nova', 'nebula', 'montana', 'mora', 'florida', 'leo', 'rana', 'octopus', 'pluto', 'java', 'antarctica', 'mars', 'io', 'lotus', 'delta', 'bella', 'niger']\n",
            "usual is a synonym of [MASK] → ['[UNK]', 'it', 'iris', 'sp', 'octopus', 'mora', 'nova', 'leo', 'm', 'p', 'montana', 'venus', 'c', 'doris', 'delta', 'helena', 'viola', 'geometridae', 'io', 'rana']\n",
            "common is a synonym of [MASK] → ['var', 'sp', 'p', 'minor', 'it', 'nova', 'm', 'b', 'major', 'c', 'mt', 'species', 'common', 'a', 'ave', 'viola', '[UNK]', 'iris', 'hon', 'n']\n",
            "ordinary is a synonym of [MASK] → ['[UNK]', 'it', 'minor', 'ana', 'mono', 'm', '†', 'nova', 'venus', 'p', 'c', 'sic', 'o', 'sp', 'bella', 'la', 'normal', 'nebula', 'b', 'negro']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzSnYko6_xG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}