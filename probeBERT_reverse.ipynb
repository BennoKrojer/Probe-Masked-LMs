{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "probeBERT_reverse.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMuR+jYAFRum3U0aGmaRbvv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BennoKrojer/Probe-Masked-LMs/blob/master/probeBERT_reverse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-Fn1Qmh2w7Z",
        "colab_type": "text"
      },
      "source": [
        "This Notebook is intended to test relations that are symmetric or inverse.\n",
        "So to test this, we will ask \"a relation []?\" and then take the prediction p and ask \"p relation []?\". If the model has learnt that the relation is symmetric (or inverse), it should then say a, regardless of whether that is now correct or not.\n",
        "Unfortunately this approach only works properly for 1-to-1 relations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCg_nBdn1HVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_q27yxX1YtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subjects = ['Austria', 'Denmark', 'Switzerland', 'Ukraine', 'Belarus', 'Estonia', 'Afghanistan', 'Mexico', 'Egypt', 'Angola', 'Honduras', 'Panama', 'Turkey', 'Belgium', 'Mongolia', 'Hungary', 'Niger']\n",
        "relation = 'is north of'\n",
        "reverse = 'is south of'\n",
        "probes = [f'{subj} {relation}' for subj in subjects]\n",
        "cased_model = True\n",
        "numb_predictions_displayed = 5\n",
        "ignore_self_reference_output = True # BERT tends to predict the subject again in many cases. This can be ignored."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni2lLPdm4vfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, tokenized_text):\n",
        "  masked_index = [i for i, x in enumerate(tokenized_text) if x == '[MASK]'][0]\n",
        "  print(f'TOKENIZED TEXT: {tokenized_text}')\n",
        "\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  segments_ids = [0]*len(tokenized_text)\n",
        "\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  # Predict all tokens\n",
        "  with torch.no_grad():\n",
        "      outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "      predictions = outputs[0][0][masked_index]\n",
        "  predicted_ids = torch.argsort(predictions, descending=True)[:numb_predictions_displayed]\n",
        "  predicted_tokens = tokenizer.convert_ids_to_tokens(list(predicted_ids))\n",
        "  return predicted_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xm2SbTa1ZMm",
        "colab_type": "code",
        "outputId": "49a4ec48-708a-47de-d13c-34ca6861e92e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM, RobertaForMaskedLM, RobertaTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# from: https://huggingface.co/transformers/quickstart.html#bert-example\n",
        "\n",
        "bert_model = 'bert-large-cased' if cased_model else 'bert-large-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
        "model = BertForMaskedLM.from_pretrained(bert_model)\n",
        "model.eval()\n",
        "\n",
        "output = {'symmetric': [], 'asymmetric': []}\n",
        "for probe in probes:\n",
        "  text = f'[CLS] {probe} [MASK] . [SEP]'\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  predicted_tokens = predict(model, tokenized_text)\n",
        "\n",
        "  first_pred = predicted_tokens[0]\n",
        "  if first_pred == tokenized_text[1]:\n",
        "    first_pred = predicted_tokens[1]\n",
        "\n",
        "  reverse_probe = f'[CLS] {first_pred} {reverse} [MASK] . [SEP]'\n",
        "  reverse_probe = tokenizer.tokenize(reverse_probe)\n",
        "  rev_predicted_tokens = predict(model, reverse_probe)\n",
        "  rev_first_pred = rev_predicted_tokens[0]\n",
        "  if rev_first_pred == reverse_probe[1]:\n",
        "    rev_first_pred = rev_predicted_tokens[1]\n",
        "  \n",
        "  if rev_first_pred == tokenized_text[1]:\n",
        "    output['symmetric'].append(((probe, predicted_tokens), (reverse_probe, rev_predicted_tokens)))\n",
        "  else:\n",
        "    output['asymmetric'].append(((probe, predicted_tokens), (reverse_probe, rev_predicted_tokens)))\n",
        "\n",
        "for key, val in output.items():\n",
        "  print(key)\n",
        "  for (probe, pred), (rev_probe, rev_pred) in val:\n",
        "    print(f'{probe} [MASK] → {pred}')\n",
        "    print(f'{rev_probe} [MASK] → {rev_pred}')\n",
        "\n",
        "  print('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKENIZED TEXT: ['[CLS]', 'Lu', '##anda', 'is', 'north', 'of', '[MASK]', '.', '[SEP]']\n",
            "TOKENIZED TEXT: ['[CLS]', 'Angola', 'is', 'south', 'of', '[MASK]', '.', '[SEP]']\n",
            "TOKENIZED TEXT: ['[CLS]', 'Jay', '-', 'Z', 'is', 'north', 'of', '[MASK]', '.', '[SEP]']\n",
            "TOKENIZED TEXT: ['[CLS]', 'town', 'is', 'south', 'of', '[MASK]', '.', '[SEP]']\n",
            "TOKENIZED TEXT: ['[CLS]', 'Urban', 'is', 'north', 'of', '[MASK]', '.', '[SEP]']\n",
            "TOKENIZED TEXT: ['[CLS]', 'Central', 'is', 'south', 'of', '[MASK]', '.', '[SEP]']\n",
            "TOKENIZED TEXT: ['[CLS]', 'Kid', '##man', 'is', 'north', 'of', '[MASK]', '.', '[SEP]']\n",
            "TOKENIZED TEXT: ['[CLS]', 'Mt', 'is', 'south', 'of', '[MASK]', '.', '[SEP]']\n",
            "TOKENIZED TEXT: ['[CLS]', 'Co', '##ba', '##in', 'is', 'north', 'of', '[MASK]', '.', '[SEP]']\n",
            "TOKENIZED TEXT: ['[CLS]', 'Mt', 'is', 'south', 'of', '[MASK]', '.', '[SEP]']\n",
            "TOKENIZED TEXT: ['[CLS]', 'Reynolds', 'is', 'north', 'of', '[MASK]', '.', '[SEP]']\n",
            "TOKENIZED TEXT: ['[CLS]', 'town', 'is', 'south', 'of', '[MASK]', '.', '[SEP]']\n",
            "TOKENIZED TEXT: ['[CLS]', 'Live', '##ly', 'is', 'north', 'of', '[MASK]', '.', '[SEP]']\n",
            "TOKENIZED TEXT: ['[CLS]', 'town', 'is', 'south', 'of', '[MASK]', '.', '[SEP]']\n",
            "symmetric\n",
            "\n",
            "\n",
            "asymmetric\n",
            "Luanda is north of [MASK] → ['Angola', 'Lisbon', 'it', 'Mozambique', 'Johannesburg']\n",
            "['[CLS]', 'Angola', 'is', 'south', 'of', '[MASK]', '.', '[SEP]'] [MASK] → ['Namibia', 'Zambia', 'Congo', 'Angola', 'Gabon']\n",
            "Jay-Z is north of [MASK] → ['town', 'Detroit', 'Atlanta', 'it', 'here']\n",
            "['[CLS]', 'town', 'is', 'south', 'of', '[MASK]', '.', '[SEP]'] [MASK] → ['town', 'city', 'village', 'it', 'river']\n",
            "Urban is north of [MASK] → ['Urban', 'Central', 'downtown', 'Austin', 'Rural']\n",
            "['[CLS]', 'Central', 'is', 'south', 'of', '[MASK]', '.', '[SEP]'] [MASK] → ['downtown', 'Central', 'Midland', 'town', 'Downtown']\n",
            "Kidman is north of [MASK] → ['Mt', 'town', 'it', 'downtown', 'Austin']\n",
            "['[CLS]', 'Mt', 'is', 'south', 'of', '[MASK]', '.', '[SEP]'] [MASK] → ['Mt', 'Mount', 'it', 'Mont', 'town']\n",
            "Cobain is north of [MASK] → ['Mt', 'town', 'it', 'downtown', 'Chicago']\n",
            "['[CLS]', 'Mt', 'is', 'south', 'of', '[MASK]', '.', '[SEP]'] [MASK] → ['Mt', 'Mount', 'it', 'Mont', 'town']\n",
            "Reynolds is north of [MASK] → ['Reynolds', 'town', 'downtown', 'Lansing', 'Hoover']\n",
            "['[CLS]', 'town', 'is', 'south', 'of', '[MASK]', '.', '[SEP]'] [MASK] → ['town', 'city', 'village', 'it', 'river']\n",
            "Lively is north of [MASK] → ['town', 'downtown', 'Austin', 'Mt', 'Kingston']\n",
            "['[CLS]', 'town', 'is', 'south', 'of', '[MASK]', '.', '[SEP]'] [MASK] → ['town', 'city', 'village', 'it', 'river']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzRLLNs763kG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}